import torch
from typing import Optional, Tuple
from transformers.models.{{ transformers_model_type }}.modeling_{{ transformers_model_type }} import {{ model_name }}DecoderLayer
{%- if needs_attention_mask_utils %}
from transformers.modeling_attn_mask_utils import _prepare_4d_causal_attention_mask
{%- endif %}

class Wrapped{{ model_name }}Block({{ model_name }}DecoderLayer):
    def __init__(self, config, layer_idx: int):
        super().__init__(config, layer_idx)
        {%- for attr in block_attributes %}
        self.{{ attr }} = config.{{ attr }}
        {%- endfor %}
    
    def forward(
        self,
        hidden_states: torch.Tensor,
        *args,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        layer_past: Optional[Tuple[torch.Tensor]] = None,
        use_cache: bool = False,
        **kwargs
    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:
        batch_size, seq_length, _ = hidden_states.shape
        
        seq_length_with_past = seq_length
        past_key_values_length = 0
        
        past_key_value = layer_past
        if past_key_value is not None:
            past_key_values_length = past_key_value[0].shape[2]
            seq_length_with_past = seq_length_with_past + past_key_values_length
            past_key_value = self._reorder_cache_from_bloom_to_{{ model_name_lower }}(past_key_value, batch_size, past_key_values_length)
        
        {%- if needs_attention_mask %}
        if attention_mask is None:
            attention_mask = torch.ones(
                (batch_size, seq_length_with_past), dtype=torch.bool, device=hidden_states.device
            )
        attention_mask = _prepare_4d_causal_attention_mask(
            attention_mask=attention_mask,
            input_shape=(batch_size, seq_length),
            inputs_embeds=hidden_states,
            past_key_values_length=past_key_values_length,
        )
        {%- endif %}
        
        outputs = super().forward(
            hidden_states,
            *args,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_value=past_key_value,
            use_cache=use_cache,
            **kwargs,
        )
        
        return outputs
    
    def _reorder_cache_from_bloom_to_{{ model_name_lower }}(
        self, key_value: Tuple[torch.Tensor], batch_size: int, seq_length: int
    ) -> Tuple[torch.Tensor]:
        key_states, value_states = key_value
        {%- if cache_reorder_pattern == "llama" %}
        key_states = key_states.permute(0, 2, 1)
        key_states = key_states.view(
            batch_size, self.self_attn.num_key_value_heads, seq_length, self.self_attn.head_dim
        )
        value_states = value_states.view(*key_states.shape)
        {%- else %}
        # TODO: Implement cache reordering for {{ model_name }}
        {%- endif %}
        return (key_states, value_states)